INFO 04-12 21:11:15 [__init__.py:239] Automatically detected platform cuda.
INFO 04-12 21:11:16 [api_server.py:1034] vLLM API server version 0.8.3
INFO 04-12 21:11:16 [api_server.py:1035] args: Namespace(subparser='serve', model_tag='/data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ', config='', host=None, port=8000, uvicorn_log_level='warning', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, download_dir=None, load_format='auto', config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', kv_cache_dtype='auto', max_model_len=10000, guided_decoding_backend='xgrammar', logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=None, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, limit_mm_per_prompt=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, use_tqdm_on_load=True, multi_step_stream_outputs=True, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_config=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, calculate_kv_scales=False, additional_config=None, enable_reasoning=False, reasoning_parser=None, disable_cascade_attn=False, disable_log_requests=True, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False, dispatch_function=<function ServeSubcommand.cmd at 0x7fbae5c499e0>)
INFO 04-12 21:11:23 [config.py:600] This model supports multiple tasks: {'reward', 'classify', 'score', 'generate', 'embed'}. Defaulting to 'generate'.
INFO 04-12 21:11:24 [awq_marlin.py:114] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
INFO 04-12 21:11:24 [config.py:1600] Defaulting to use mp for distributed inference
INFO 04-12 21:11:24 [config.py:1780] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 04-12 21:11:27 [__init__.py:239] Automatically detected platform cuda.
INFO 04-12 21:11:29 [core.py:61] Initializing a V1 LLM engine (v0.8.3) with config: model='/data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ', speculative_config=None, tokenizer='/data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=/data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
WARNING 04-12 21:11:29 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 112 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 04-12 21:11:29 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 10485760, 10, 'psm_1f9b73da'), local_subscribe_addr='ipc:///tmp/bcd40457-b420-4def-bae3-62bab33a1f53', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 04-12 21:11:32 [__init__.py:239] Automatically detected platform cuda.
WARNING 04-12 21:11:35 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9e48bd38c0>
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:35 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_eb624d33'), local_subscribe_addr='ipc:///tmp/4146cea1-aafe-49b4-b193-6abe3ca0f740', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 04-12 21:11:38 [__init__.py:239] Automatically detected platform cuda.
WARNING 04-12 21:11:40 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7ff6a0c945f0>
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:40 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dcf0b6b4'), local_subscribe_addr='ipc:///tmp/8a895f52-d90c-436e-9bdb-12ad092fa38b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[rank1]:[W412 21:11:40.215125840 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
INFO 04-12 21:11:43 [__init__.py:239] Automatically detected platform cuda.
WARNING 04-12 21:11:45 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fcbe9b7a510>
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:45 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_39a10678'), local_subscribe_addr='ipc:///tmp/ba757761-725a-45c5-b733-3f8b30c4fd27', remote_subscribe_addr=None, remote_addr_ipv6=False)
[rank2]:[W412 21:11:46.525273192 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
INFO 04-12 21:11:48 [__init__.py:239] Automatically detected platform cuda.
WARNING 04-12 21:11:51 [utils.py:2413] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0758e688f0>
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_df8d059b'), local_subscribe_addr='ipc:///tmp/87467ab8-b8ca-4ece-9232-70ee36b662ce', remote_subscribe_addr=None, remote_addr_ipv6=False)
[rank3]:[W412 21:11:51.729704987 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[rank0]:[W412 21:11:51.737689544 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [utils.py:990] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:51 [utils.py:990] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:51 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:51 [utils.py:990] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [utils.py:990] Found nccl from library libnccl.so.2
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:51 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [pynccl.py:69] vLLM is using nccl==2.21.5
[1;36m(VllmWorker rank=2 pid=52941)[0;0m WARNING 04-12 21:11:51 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=3 pid=52965)[0;0m WARNING 04-12 21:11:51 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m WARNING 04-12 21:11:51 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=1 pid=52896)[0;0m WARNING 04-12 21:11:51 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [shm_broadcast.py:264] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0d5318a2'), local_subscribe_addr='ipc:///tmp/46c9db61-7881-48eb-8c14-a20a47aa677d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [parallel_state.py:957] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [parallel_state.py:957] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:51 [gpu_model_runner.py:1258] Starting to load model /data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ...
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:51 [gpu_model_runner.py:1258] Starting to load model /data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ...
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:51 [parallel_state.py:957] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:51 [parallel_state.py:957] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:51 [cuda.py:221] Using Flash Attention backend on V1 engine.
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:51 [gpu_model_runner.py:1258] Starting to load model /data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ...
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:51 [gpu_model_runner.py:1258] Starting to load model /data/lht/ckpt/Qwen2.5-32B-Instruct-AWQ...
[1;36m(VllmWorker rank=3 pid=52965)[0;0m WARNING 04-12 21:11:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m WARNING 04-12 21:11:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]
[1;36m(VllmWorker rank=2 pid=52941)[0;0m WARNING 04-12 21:11:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=1 pid=52896)[0;0m WARNING 04-12 21:11:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  3.39it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:00<00:00,  3.06it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:00<00:00,  2.95it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:01<00:00,  3.05it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  3.48it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:01<00:00,  3.28it/s]
[1;36m(VllmWorker rank=0 pid=52844)[0;0m 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:53 [loader.py:447] Loading weights took 1.55 seconds
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:53 [loader.py:447] Loading weights took 1.76 seconds
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:53 [loader.py:447] Loading weights took 1.78 seconds
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:54 [loader.py:447] Loading weights took 1.88 seconds
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:11:54 [gpu_model_runner.py:1273] Model loading took 4.5471 GiB and 2.325502 seconds
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:11:54 [gpu_model_runner.py:1273] Model loading took 4.5471 GiB and 2.449299 seconds
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:11:54 [gpu_model_runner.py:1273] Model loading took 4.5471 GiB and 2.525463 seconds
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:11:54 [gpu_model_runner.py:1273] Model loading took 4.5471 GiB and 2.582763 seconds
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:12:12 [backends.py:416] Using cache directory: /usr/local/app/.cache/vllm/torch_compile_cache/25ae7fb61c/rank_2_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:12:12 [backends.py:426] Dynamo bytecode transform time: 17.99 s
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:12:12 [backends.py:416] Using cache directory: /usr/local/app/.cache/vllm/torch_compile_cache/25ae7fb61c/rank_3_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:12:12 [backends.py:426] Dynamo bytecode transform time: 18.08 s
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:12:12 [backends.py:416] Using cache directory: /usr/local/app/.cache/vllm/torch_compile_cache/25ae7fb61c/rank_1_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:12:12 [backends.py:426] Dynamo bytecode transform time: 18.15 s
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:12:13 [backends.py:416] Using cache directory: /usr/local/app/.cache/vllm/torch_compile_cache/25ae7fb61c/rank_0_0 for vLLM's torch.compile
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:12:13 [backends.py:426] Dynamo bytecode transform time: 18.31 s
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:12:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:12:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:12:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:12:14 [backends.py:115] Directly load the compiled graph for shape None from the cache
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:12:28 [monitor.py:33] torch.compile takes 18.15 s in total
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:12:28 [monitor.py:33] torch.compile takes 17.99 s in total
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:12:28 [monitor.py:33] torch.compile takes 18.08 s in total
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:12:28 [monitor.py:33] torch.compile takes 18.31 s in total
INFO 04-12 21:12:33 [kv_cache_utils.py:578] GPU KV cache size: 154,096 tokens
INFO 04-12 21:12:33 [kv_cache_utils.py:581] Maximum concurrency for 10,000 tokens per request: 15.41x
INFO 04-12 21:12:33 [kv_cache_utils.py:578] GPU KV cache size: 154,096 tokens
INFO 04-12 21:12:33 [kv_cache_utils.py:581] Maximum concurrency for 10,000 tokens per request: 15.41x
INFO 04-12 21:12:33 [kv_cache_utils.py:578] GPU KV cache size: 154,096 tokens
INFO 04-12 21:12:33 [kv_cache_utils.py:581] Maximum concurrency for 10,000 tokens per request: 15.41x
INFO 04-12 21:12:33 [kv_cache_utils.py:578] GPU KV cache size: 154,096 tokens
INFO 04-12 21:12:33 [kv_cache_utils.py:581] Maximum concurrency for 10,000 tokens per request: 15.41x
[1;36m(VllmWorker rank=0 pid=52844)[0;0m INFO 04-12 21:13:45 [gpu_model_runner.py:1608] Graph capturing finished in 72 secs, took 2.23 GiB
[1;36m(VllmWorker rank=1 pid=52896)[0;0m INFO 04-12 21:13:45 [gpu_model_runner.py:1608] Graph capturing finished in 72 secs, took 2.24 GiB
[1;36m(VllmWorker rank=2 pid=52941)[0;0m INFO 04-12 21:13:45 [gpu_model_runner.py:1608] Graph capturing finished in 72 secs, took 2.24 GiB
[1;36m(VllmWorker rank=3 pid=52965)[0;0m INFO 04-12 21:13:45 [gpu_model_runner.py:1608] Graph capturing finished in 72 secs, took 2.23 GiB
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] WorkerProc hit an exception: %s
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 2 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901648 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 2 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901648 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=2 pid=52941)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] WorkerProc hit an exception: %s
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 1 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901433 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 1 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901433 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=1 pid=52896)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] WorkerProc hit an exception: %s
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901251 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampler_output = self.model.sample(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     next_tokens = self.sampler(logits, sampling_metadata)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     sampled = self.sample(logits, sampling_metadata)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     random_sampled = self.topk_topp_sampler(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                      ^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return self._call_impl(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return forward_call(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits = apply_top_k_top_p(logits, k, p)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901251 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] The above exception was the direct cause of the following exception:
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] Traceback (most recent call last):
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     output = func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]              ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     self.model_runner._dummy_sampler_run(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     return func(*args, **kwargs)
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]            ^^^^^^^^^^^^^^^^^^^^^
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383]     raise RuntimeError(
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[1;36m(VllmWorker rank=0 pid=52844)[0;0m ERROR 04-12 21:13:47 [multiproc_executor.py:383] 
ERROR 04-12 21:13:47 [core.py:390] EngineCore hit an exception: Traceback (most recent call last):
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 378, in run_engine_core
ERROR 04-12 21:13:47 [core.py:390]     engine_core = EngineCoreProc(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 319, in __init__
ERROR 04-12 21:13:47 [core.py:390]     super().__init__(vllm_config, executor_class, log_stats)
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 71, in __init__
ERROR 04-12 21:13:47 [core.py:390]     self._initialize_kv_caches(vllm_config)
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 159, in _initialize_kv_caches
ERROR 04-12 21:13:47 [core.py:390]     self.model_executor.initialize_from_config(kv_cache_configs)
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 63, in initialize_from_config
ERROR 04-12 21:13:47 [core.py:390]     self.collective_rpc("compile_or_warm_up_model")
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 134, in collective_rpc
ERROR 04-12 21:13:47 [core.py:390]     raise e
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 123, in collective_rpc
ERROR 04-12 21:13:47 [core.py:390]     raise result
ERROR 04-12 21:13:47 [core.py:390] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
ERROR 04-12 21:13:47 [core.py:390] Traceback (most recent call last):
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1466, in _dummy_sampler_run
ERROR 04-12 21:13:47 [core.py:390]     sampler_output = self.model.sample(
ERROR 04-12 21:13:47 [core.py:390]                      ^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/model_executor/models/qwen2.py", line 480, in sample
ERROR 04-12 21:13:47 [core.py:390]     next_tokens = self.sampler(logits, sampling_metadata)
ERROR 04-12 21:13:47 [core.py:390]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
ERROR 04-12 21:13:47 [core.py:390]     return self._call_impl(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
ERROR 04-12 21:13:47 [core.py:390]     return forward_call(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 49, in forward
ERROR 04-12 21:13:47 [core.py:390]     sampled = self.sample(logits, sampling_metadata)
ERROR 04-12 21:13:47 [core.py:390]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/sampler.py", line 115, in sample
ERROR 04-12 21:13:47 [core.py:390]     random_sampled = self.topk_topp_sampler(
ERROR 04-12 21:13:47 [core.py:390]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
ERROR 04-12 21:13:47 [core.py:390]     return self._call_impl(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
ERROR 04-12 21:13:47 [core.py:390]     return forward_call(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 98, in forward_native
ERROR 04-12 21:13:47 [core.py:390]     logits = apply_top_k_top_p(logits, k, p)
ERROR 04-12 21:13:47 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/sample/ops/topk_topp_sampler.py", line 186, in apply_top_k_top_p
ERROR 04-12 21:13:47 [core.py:390]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
ERROR 04-12 21:13:47 [core.py:390]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.74 GiB. GPU 0 has a total capacity of 22.20 GiB of which 1.64 GiB is free. Process 2901251 has 20.55 GiB memory in use. Of the allocated memory 17.84 GiB is allocated by PyTorch, with 78.00 MiB allocated in private pools (e.g., CUDA Graphs), and 132.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ERROR 04-12 21:13:47 [core.py:390] 
ERROR 04-12 21:13:47 [core.py:390] The above exception was the direct cause of the following exception:
ERROR 04-12 21:13:47 [core.py:390] 
ERROR 04-12 21:13:47 [core.py:390] Traceback (most recent call last):
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 376, in worker_busy_loop
ERROR 04-12 21:13:47 [core.py:390]     output = func(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]              ^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 226, in compile_or_warm_up_model
ERROR 04-12 21:13:47 [core.py:390]     self.model_runner._dummy_sampler_run(
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
ERROR 04-12 21:13:47 [core.py:390]     return func(*args, **kwargs)
ERROR 04-12 21:13:47 [core.py:390]            ^^^^^^^^^^^^^^^^^^^^^
ERROR 04-12 21:13:47 [core.py:390]   File "/data/miniforge/envs/lht/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1470, in _dummy_sampler_run
ERROR 04-12 21:13:47 [core.py:390]     raise RuntimeError(
ERROR 04-12 21:13:47 [core.py:390] RuntimeError: CUDA out of memory occurred when warming up sampler with 1024 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
ERROR 04-12 21:13:47 [core.py:390] 
ERROR 04-12 21:13:47 [core.py:390] 
CRITICAL 04-12 21:13:47 [core_client.py:361] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.
